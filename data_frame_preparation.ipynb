{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/corvusMidnight/thesis/blob/main/data_frame_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thXugXq0o4Yy"
      },
      "source": [
        "# A notebook for feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzoZLaCOo4Y7"
      },
      "outputs": [],
      "source": [
        "##### IMPORTS #####\n",
        "%%capture\n",
        "!pip install -r requirements.txt | grep -v 'already satisfied'\n",
        "!pip install demoji\n",
        "\n",
        "!pip install lazypredict\n",
        "!pip install plotly\n",
        "!pip install emoji\n",
        "!pip install tokenizer\n",
        "!pip install transformers\n",
        "!pip install simpletransformers\n",
        "!pip install happytransformer\n",
        "\n",
        "#Basic imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import sklearn\n",
        "import re\n",
        "import seaborn as sns\n",
        "import string\n",
        "import operator\n",
        "import plotly.express as px\n",
        "from collections import Counter\n",
        "from time import time\n",
        "import pickle\n",
        "from scipy import stats\n",
        "import demoji\n",
        "\n",
        "#NLP imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "#Baseline with LazyClassifier\n",
        "from lazypredict.Supervised import LazyClassifier, LazyRegressor\n",
        "\n",
        "#nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopword=set(stopwords.words('italian'))\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import word_tokenize, FreqDist\n",
        "stemmer = nltk.SnowballStemmer(\"italian\")\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk import ngrams, FreqDist\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Classifiers\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "#Evaluation tools\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import learning_curve, validation_curve\n",
        "from sklearn.metrics import make_scorer\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "\n",
        "\n",
        "#Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "#Pipeline visualization\n",
        "from sklearn import set_config\n",
        "from sklearn.utils import estimator_html_repr\n",
        "\n",
        "#Pipeline display mode\n",
        "set_config(display='diagram')\n",
        "\n",
        "\n",
        "#Imputers\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "#Scalers\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import unicodedata as uni\n",
        "from google.colab import drive \n",
        "import emoji\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from tokenizer import *\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "#Model tuning\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
        "\n",
        "#Neural and pretrained models\n",
        "import torch\n",
        "from torch import nn \n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
        "from happytransformer import HappyTextClassification\n",
        "\n",
        "from datasets import Dataset\n",
        "from datasets import load_metric\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "from transformers import TrainingArguments, AutoModelForSequenceClassification, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs8UnYASo4ZA"
      },
      "source": [
        "## Linguistic standardness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZW_lum3o4ZC"
      },
      "outputs": [],
      "source": [
        "# this file contains functions that can be imported in other py.scripts\n",
        "\n",
        "abbrev_dict = {\n",
        "'aspetta':['asp', 'aspe'],\n",
        "'anche':'anke',\n",
        "'bene':'bn',\n",
        "'birthday':['bday', 'b-day'],\n",
        "'boyfriend':['bf', 'boy'],\n",
        "'bravo':'brv',\n",
        "'capito':'cpt',\n",
        "'che':'ke',\n",
        "'ci sei':['c6', 'c 6'],\n",
        "'come':'cm',\n",
        "'comunque':'cmq',\n",
        "'con':'cn',\n",
        "'cosa': 'cs',\n",
        "'dev':'developer',\n",
        "'easy':'EZ',\n",
        "'Facebook':'fb',\n",
        "'forever':'4ever',\n",
        "'fuck':['fck','fk'],\n",
        "'fucking':['fcking','fking'],\n",
        "'grazie':['gratz', 'gz'],\n",
        "'great':'gr8',\n",
        "\"I don't know\":'dunno',\n",
        "'inderdaad':'idd',\n",
        "'Instagram': 'insta',\n",
        "'messaggio':['mex','messa'],\n",
        "'nevermind':'nvm',\n",
        "'non':'nn',\n",
        "'niente':'nnt',\n",
        "'thanks':['thx', 'tx','thks', 'thnx'],\n",
        "'please':'plz',\n",
        "'per sempre':'xseo',\n",
        "'perché': ['xche', 'xché', 'xchè'],\n",
        "'qualcuno':'qcn',\n",
        "'rosicare':'rosik',\n",
        "'scusa':'scs',\n",
        "'sei un':'6 1',\n",
        "'sorry':['sry', 'srry'],\n",
        "'ti voglio bene per sempre': ['tvbxseo'],\n",
        "'too easy':'2EZ',\n",
        "'tremendo':'3mendo',\n",
        "'va bene':['vbne', 'vbbe', 'vbbè'],\n",
        "'vaffanculo':['vaffa', 'fanculo'],\n",
        "'what':['wat', 'wht', 'wut'],\n",
        "}\n",
        "\n",
        "\n",
        "acronym_dict_EN = {\n",
        "'also known as':'AKA',\n",
        "'as soon as possible':'asap',\n",
        "'at the moment': 'atm',\n",
        "'away from keyboard':'AFK',\n",
        "'be right back':'brb',\n",
        "'best friend for life':'bffl',\n",
        "'best friend forever':'bff',\n",
        "'by the way':'btw',\n",
        "'for the win':'ftw',\n",
        "'fuck my life':'fml',\n",
        "\"I don't know\":'idk',\n",
        "'I know, right' : 'ikr',\n",
        "'I love you': 'ily',\n",
        "'in my humble opinion':'imho',\n",
        "'in my opinion':'imo',\n",
        "'in real life':'irl',\n",
        "'laughing my ass off':'lmao',\n",
        "'laughing my fucking ass off':'lmfao',\n",
        "'laughing out loud':'lol',\n",
        "'love you so much':'lysm',\n",
        "'love you':'ly',\n",
        "'love you so much': 'lysm',\n",
        "'never want to lose you': \"nwly\",\n",
        "'no problem': \"np\",\n",
        "'oh my god':'omg',\n",
        "'rolling on the floor laughing':'rofl',\n",
        "'see you':'cu',\n",
        "'shut the fuck up':'stfu',\n",
        "'talk to you later':'ttyl',\n",
        "'to be honest':'tbh',\n",
        "'what the fuck':'wtf',\n",
        "'what the hell':'wth',\n",
        "'you are welcome' : \"yw\",\n",
        "'you only live once':'yolo'}\n",
        "\n",
        "\n",
        "acronym_dict_IT = {\n",
        "'cresci bene che ripasso':'cbcr',\n",
        "'figli di puttana':'fdp',\n",
        "'ti amo di bene': ['tadb'],\n",
        "'ti voglio bene': ['tvb']}\n",
        "\n",
        "##### FEATURE: EMOTICONS #####\n",
        "\n",
        "\n",
        "# we define 'normal' unicode chars, i.e. non-emoji\n",
        "###normal_unicode = ('VARIATION', 'SELECTOR', 'SELECTOR-16', 'GREEK', 'ARROW', 'HANGUL', 'DAGGER','LATIN', 'DOT', 'MACRON', 'DIFFERENTIAL', 'ELLIPSIS', 'ORDINAL','INDICATOR', 'DASH', 'TILDE','DIAERESIS','APOSTROPHE','LINE','LETTER', 'SPACE', 'DIGIT', 'HYPHEN-MINUS', 'COMMA', 'COLON', 'AMPERSAND', 'COMMERCIAL','STOP', 'ACCENT', 'PARENTHESIS', 'SOLIDUS', 'MARK', 'FULL STOP','SEMICOLON','ASTERISK', 'BRACKET','SIGN')\n",
        "normal_unicode = ('DRAWINGS', 'BLOCK', 'SYLLABLE', 'HANGUL', 'QUOTATION', 'MINUS', 'COMBINING', 'INDICATOR', 'HEBREW', 'CENT', 'TILDE', 'ORDINAL', 'DIFFERENTIAL', 'COMMA', 'LATIN', 'COLON',  'VOWEL', 'NUMERO', 'APOSTROPHE', 'LESS-THAN', 'AFGHANI', 'LETTER', 'PARENTHESIS', 'SELECTOR-16', 'AMPERSAND', 'DEGREE', 'HYPHEN-MINUS', 'EURO', 'PLUS-MINUS', 'ACCENT', 'OHM', 'BRACKET', 'ARABIC', 'SUPERSCRIPT', 'NOMISMA', 'SECTION', 'QUESTION', 'ELLIPSIS', 'DAGGER', 'COMMERCIAL', 'EURO-CURRENCY', 'MINUS-OR-PLUS', 'CYRILLIC', 'ASTERISK', 'GREATER-THAN', 'CURRENCY', 'EQUALS', 'PERSIAN', 'STOP', 'PERCENT', 'SUBSCRIPT', 'TRADE', 'DIAERESIS', 'LINEAR', 'LINE', 'PER', 'DOT', 'DASH', 'COPYRIGHT', 'DIVISION', 'SOLIDUS', 'ARMENIAN', 'MICRO', 'TAI', 'SPACE', 'PERSIAN', 'DIGIT', 'KELVIN', 'SEMICOLON', 'NUMBER', 'PLUS', 'OUNCE', 'ROMAN', 'POUND', 'MULTIPLICATION', 'GREEK', 'EXCLAMATION', 'SELECTOR', 'DOLLAR', 'MODIFIER', 'VARIATION')\n",
        "\n",
        "\n",
        "# remark: 'VARIATION SELECTOR-16': ️ Variation Selector-16:\n",
        "# \"An invisible codepoint which specifies that the preceding character should be displayed with emoji presentation. Only required if the preceding character defaults to text presentation.\"\n",
        "\n",
        "# remark2: 'ARROW' eruit gehaald, want te veel smileys met een pijl, en weinig echte pijlen in CMC\n",
        "\n",
        "facebook_faces = [\"smile\",\n",
        "\"frown\",\n",
        "\"unsure\",\n",
        "\"grin\",\n",
        "\"tongue\",\n",
        "\"wink\",\n",
        "\"gasp\",\n",
        "\"upset\",\n",
        "\"cry\",\n",
        "\"confused_rev\",\n",
        "\"confused\",\n",
        "\"grumpy\",\n",
        "\"confused\",\n",
        "\"glasses\",\n",
        "\"sunglasses\",\n",
        "\"devil\",\n",
        "\"angel\",\n",
        "\"kiss\",\n",
        "\"kiki\",\n",
        "\"squint\",\n",
        "\"pacman\",\n",
        "\"colonthree\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### REGEXES #####\n",
        "\n",
        "punct = re.compile(r'[\\?\\!,.:;()\"\\*\\s]+') \n",
        "# remark: \\s added to punct in order to still split on whitespace too\n",
        "# remark: a single quotation mark isn't added, in order to still detect abbreviations like 'k, 't, ...\n",
        "\n",
        "\n",
        "##### FUNCTIONS #####\n",
        "\n",
        "def split_textNEW(text):\n",
        "\t\"\"\"\n",
        "\tfunction to split a text into tokens in slightly different ways\n",
        "\n",
        "\tinput:\n",
        "\t\t- 'text': a text string\n",
        "\n",
        "\toutput:\n",
        "\t\t- lists of:\n",
        "\t\t\t- tokens with punct attached\n",
        "\t\t\t- tokens without punct\n",
        "\t\t\t- lowercased tokens without punct\n",
        "\t\t\t- lowercased tokens with punct\n",
        "\t\t- the lowercased text\n",
        "\t\"\"\"\n",
        "\n",
        "\t# we also create a lowercased version of the text\n",
        "\ttext_lower = text.lower()\n",
        "\n",
        "\t# we split both versions\n",
        "\t# ==> tokens: words with poss. punctuation attached\n",
        "\ttokens = [item for item in text.split() if item]\n",
        "\ttokens_lower = [item for item in text_lower.split() if item]\n",
        "\n",
        "\t# ==> clean_tokens: words without poss. punctuation attached\n",
        "\tclean_tokens = [item for item in re.split(punct,text) if item]\n",
        "\tclean_tokens_lower = [item for item in re.split(punct,text_lower) if item]\n",
        "\n",
        "\treturn(tokens, clean_tokens, clean_tokens_lower, tokens_lower, text_lower)\n",
        "\n",
        "\"\"\"script to count several non-standard Italian features in a social media corpus\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "##### REGEXES #####\n",
        "\n",
        "numbers_and_ordinals = re.compile(r'^\\d+(de|st|ste|e)?$')\n",
        "ordinals_EN = re.compile(r'^\\d+(nd|th|rd)$')\n",
        "hyperlink = re.compile(r'(http:\\/\\/)?(www\\.)?\\w+\\.(com|be|org|nl|ac|fm|gov|me|net|webs|de|co|uk|it|fr|to|nu|tk).*')\n",
        "email = re.compile(r'[a-zA-Z0-9-_.]+@\\w+\\.\\w+')\n",
        "hour_refs = re.compile(r'^\\d{1,2}u(\\d{2})?$')\n",
        "hashtags = re.compile(r'^#\\w+$')\n",
        "ats = re.compile(r'^@[a-zA-Z]+:?$')\n",
        "files = re.compile(r'^.+\\.(doc|docx|pdf|ppt|pptx|xml|jpg|png|jpeg|psd)$')\n",
        "money = re.compile(r'^\\d+(€|\\$|£)$')\n",
        "pages = re.compile(r'^(p|P)\\.?\\d+(\\-\\d+)?$')\n",
        "meter = re.compile(r'^\\d+k?m$')\n",
        "\n",
        "western_emoticons_all = re.compile(r'(<?[:;xX\\|=8]\\'?[-^o]?[\\)\\(xXpPSsdDoOvV\\|\\/\\*$@#\\[\\]])|([\\)\\(][-^o]?\\'?[:;]>?)') # including some MSN emoticons consisting of letters and punct marks\n",
        "not_western_emoticons = re.compile(r'(\\w)\\1{1,}')\n",
        "flooding_x = re.compile(r'^x+$')\n",
        "flooding_xo = re.compile(r'^(x+o+)+x?$')\n",
        "\n",
        "\n",
        "##### FUNCTIONS FOR FEATURE DETECTION #####\n",
        "\n",
        "\n",
        "##### FEATURE: FLOODING #####\n",
        "\n",
        "\n",
        "def letter_flooding(tokens):\n",
        "\t\"\"\"\n",
        "\tthis function analyzes the use of letter and punctuation flooding in a text, i.e. the repetition of a character\n",
        "\n",
        "\tinput:\n",
        "\t\t- 'tokens': a text split on whitespaces (i.e. a list of the tokens of a text)\n",
        "\n",
        "\toutput:\n",
        "\t\t- the absolute number of (words containing) letter flooding in the text\n",
        "\t\t- the relative number of (words containing) letter flooding in the text\n",
        "\t\t- a freq dict of the words containing letter flooding\n",
        "\t\"\"\"\n",
        "\n",
        "\t# we create containers to store the flooding occurrences\n",
        "\tflooding_letters = defaultdict(int)\n",
        "\n",
        "\t# we define letter flooding\n",
        "\t# letter flooding: repetition of three or more times the same letter (as a repetition of two times the same letter occurs frequently in Standard Dutch words)\n",
        "\tflooding_alpha = re.compile(r'([a-z])\\1{2,}')\n",
        "\t# we will, however, not count the repetition of the letter 'x' or the combi 'xo' as flooding, as this repetition does not represent a lengthening of the sound /ks/\n",
        "\tflooding_x = re.compile(r'^x+$')\n",
        "\tflooding_xo = re.compile(r'^(x+o+)+x?$')\n",
        "\n",
        "\t# we store the occurrences of flooding\n",
        "\t# we only want to work with clean tokens\n",
        "\n",
        "\tclean_tokens = []\n",
        "\n",
        "\tfor token in tokens:\n",
        "\t\ttoken_lower = token.lower()\n",
        "\n",
        "\t\t# we do not want to count hyperlinks and emoticons as flooding\n",
        "\t\tif not (hyperlink.search(token_lower) or email.search(token_lower) or (emoticons([token])[0] > 0)):\n",
        "\t\t\tclean = [item.lower() for item in re.compile(r'\\W+').split(token) if item != \"\"]\n",
        "\t\t\tclean_tokens += clean\n",
        "\n",
        "\tfor token in clean_tokens:\n",
        "\n",
        "\t\tif flooding_alpha.search(token.lower()):\n",
        "\t\t\t# we add 1 to a general counter per flooded letter\n",
        "\t\t\t# we do not want to detect flooding of the letter x\n",
        "\t\t\tflooded_letters_without_x = [letter for letter in flooding_alpha.findall(token) if letter.lower() != 'x']\n",
        "\t\t\tif len(flooded_letters_without_x) != 0:\n",
        "\t\t\t\tfor letter in flooded_letters_without_x:\n",
        "\t\t\t\t\tflooding_letters[letter.lower()+'-variants'] += 1\n",
        "\t\t\t\t# we store the clean token\n",
        "\t\t\t\tflooding_letters[token.lower()] += 1\n",
        "\n",
        "\n",
        "\t# all flooded words:\n",
        "\tnumber_flooding_letters = sum([flooding_letters[key] for key in flooding_letters if not key.endswith('-variants')])\n",
        "\t#number_flooding_letters = sum(flooding_letters.values())\n",
        "\t# the percentage of flooded words:\n",
        "\tif len(tokens) != 0:\n",
        "\t\tpercentage_flooding_letters = float(number_flooding_letters)/len(tokens)\n",
        "\telse:\n",
        "\t\tpercentage_flooding_letters = 0\n",
        "\n",
        "\t# we return these relative numbers and freq dicts\n",
        "\treturn(number_flooding_letters,percentage_flooding_letters,flooding_letters)\n",
        "\n",
        "\n",
        "def punct_flooding(tokens):\n",
        "\t\"\"\"\n",
        "\tthis function analyzes the use of punctuation flooding in a text, i.e. the repetition of a punctuation mark\n",
        "\n",
        "\tinput:\n",
        "\t\t- 'tokens': a text split on whitespaces (i.e. a list of the tokens of a text)\n",
        "\n",
        "\toutput:\n",
        "\t\t- the absolute number of punctuation flooding\n",
        "\t\t- the relative number of punctuation flooding\n",
        "\t\t- a freq dict of the flooded punctuation marks\n",
        "\t\t- the number of (groups of) punctuation marks in the text\n",
        "\t\"\"\"\n",
        "\n",
        "\t# we create containers to store the flooding occurrences\n",
        "\tflooding_punctuation = defaultdict(int)\n",
        "\n",
        "\t# punctuation flooding: repetition of two or more times a question or exclamation mark\n",
        "\tflooding_nonalphanumeric = re.compile(r'(\\?|!)\\1{1,}')\n",
        "\t\"\"\"# in the Netlog data, letters with accent marks are often replaced by '???' we do not want to count this as punct flooding\n",
        "\tnot_flooding_nonalphanumeric = re.compile(r'\\w+(\\?){3}\\w+')\"\"\" # not applicable here\n",
        "\n",
        "\t# we only want to store the actual punctuation marks and not the letters attached\n",
        "\tclean_punc = []\n",
        "\n",
        "\tfor token in tokens:\n",
        "\t\tclean = [item for item in re.compile(r'\\w+').split(token) if item != \"\"]\n",
        "\t\tclean_punc += clean\n",
        "\n",
        "\n",
        "\t# we store the occurrences of flooding\n",
        "\tfor token in clean_punc:\n",
        "\t\tif flooding_nonalphanumeric.search(token):\n",
        "\t\t\t# we add 1 to the general counter per punct mark\n",
        "\t\t\tfor punctmark in flooding_nonalphanumeric.findall(token):\n",
        "\t\t\t\tflooding_punctuation[punctmark+'-variants'] += 1\n",
        "\t\t\t# we store the occurrence\n",
        "\t\t\tflooding_punctuation[token] += 1\n",
        "\n",
        "\t# we will need to compare the number of flooded punct.marks to the total amount of these punct.marks\n",
        "\t# we define and count all (groups of) non-flooded question and exclamation marks\n",
        "\tnr_nonflooded_punct = 0\n",
        "\t# single question and excl. marks:\n",
        "\tsingle_punct = re.compile(r'^[\\?!]$')\n",
        "\t#single_punct_after_word = re.compile(r'\\w[\\?!]')\n",
        "\t# unconv. combinations of question and excl. marks: (e.g. 'hello?!?!')\n",
        "\t# we only count the combinations in which no flooding is present (e.g. we do count '?!?!' but not '?!??', as it is already counted)\n",
        "\t#unconv_combis_without_flooding_regex = re.compile(r'^((!\\?)+!?)|((\\?!)+\\??)$')\n",
        "\tunconv_combis_without_flooding_regex = re.compile(r'^(?:(?:\\!\\?)+!?)|(?:(?:\\?\\!)+\\??)$')\n",
        "\t#unconv_combis_without_flooding_regex_after_word = re.compile(r'\\w+((!\\?)+!?)|((\\?!)+\\??)')\n",
        "\t#unconv_combis_without_flooding_regex_after_word = re.compile(r'\\w+(?:(?:(?:\\!\\?)+\\!?)|(?:(?:\\?\\!)+\\??))$')\n",
        "\n",
        "\tfor token in clean_punc:\n",
        "\t\tif (single_punct.match(token) or unconv_combis_without_flooding_regex.match(token)):\n",
        "\t\t\tnr_nonflooded_punct += 1\n",
        "\n",
        "\t# now we can calculate the total nr of occurrences of excl. and question marks\n",
        "\tnumber_flooding_punct = sum([flooding_punctuation[key] for key in flooding_punctuation if not key.endswith('-variants')])\n",
        "\ttotal_punct = nr_nonflooded_punct + number_flooding_punct\n",
        "\n",
        "\t# the percentage of flooded punctuation:\n",
        "\tif total_punct != 0:\n",
        "\t\tpercentage_flooding_punct = float(number_flooding_punct)/total_punct\n",
        "\telse:\n",
        "\t\tpercentage_flooding_punct = 0\n",
        "\n",
        "\t# we return these relative numbers and freq dicts\n",
        "\treturn(number_flooding_punct,percentage_flooding_punct,flooding_punctuation, total_punct)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def emoticons(tokens):\n",
        "\t\"\"\"\n",
        "\tthis function analyzes the use of emoticons in a text\n",
        "\n",
        "\tinput:\n",
        "\t\t- 'tokens': a text split on whitespaces (i.e. a list of the tokens of a text)\n",
        "\n",
        "\toutput:\n",
        "\t\t- the absolute number of emoticons in the text\n",
        "\t\t- the relative number of emoticons in the text\n",
        "\t\t- a freq dict of the emoticons\n",
        "\t\"\"\"\n",
        "\n",
        "\t# we define different sorts of emoticons\n",
        "\t# we also detect emoticons that may be attached to words, to make sure not to miss any (e.g. 'haha:D')\n",
        "\n",
        "\t### Western emoticons ### (in two directions, e.g. ':)' and '(:')\n",
        "\twestern_emoticons_all = re.compile(r'(<?[:;xX\\|=8]\\'?[-^o]?[\\)\\(xXpPSsdDoOvV\\|\\/\\*$@#\\[\\]])|([\\)\\(][-^o]?\\'?[:;]>?)') # including some MSN emoticons consisting of letters and punct marks\n",
        "\t# however, we do not want to detect e.g. 'xx' or 'oo' as smileys\n",
        "\tnot_western_emoticons = re.compile(r'(\\w)\\1{1,}')\n",
        "\n",
        "\t# neither do we want to detect smileys as 'XO' in kisses like 'xoxoxo'\n",
        "\tflooding_x = re.compile(r'^x+$')\n",
        "\tflooding_xo = re.compile(r'^(x+o+)+x?$')\n",
        "\t# we also do not want to recognize smileys in hyperlinks (e.g. ':/' is no smiley in this situation: https:/www. ...)\n",
        "\thyperlink = re.compile(r'(http:\\/\\/)?(www\\.)?\\w+\\.(com|be|org|nl|ac|fm|gov|me|net|webs|de|co|uk|it|fr|to|nu|tk).*')\n",
        "\n",
        "\t### Asian emoticons ###\n",
        "\tasian_emoticons = re.compile(r'(\\^_*\\^)|(\\^o?\\^)|([oO0]_+[oO0])|(T_+T)|([\\-\\—]_+[\\-\\—])|(\\-\\.\\-)|(_O_)|(<_+<)|(<\\.<)|(>_+>)|(>\\.>)|(n_+n)|(n\\.n)|(n,n)|(u_+u)|(u\\.u)|(u,u)')\n",
        "\n",
        "\t### Hearts ###\n",
        "\thearts = re.compile(r'(<+3+)|(x3+)|(X3+)')\n",
        "\n",
        "\t### typical MSN emoticons #####\n",
        "\t# LH not relevant in this corpus\n",
        "\t#MSN_emoticons = re.compile(r'\\([hHaAlLuUkKnNyY]\\)')\n",
        "\n",
        "\t### typical Netlog emoticons ###\n",
        "\t# LH not relevant in this corpus\n",
        "\t#Netlog_emoticons = re.compile(r'\\[(?:(?:@[a-zA-Z0-9\\-]+)|(?:#\\w+(_anim)?)|(?: Love )|(?: blush )|(?: dohh )|(?: hug )|(?: whistle )|(?: x )|(?:Crazylove)|(?:Flash)|(?:Hug)|(?:theband)|(?:thebonk)|(?:theflasher)|(?:theguitare)|(?:thehappyherman)|(?:theheadphone)|(?:unsure)|(?: girly )|(?: love  )|(?:LOVE)|(?:thejesus)|(?:thedancer)|(?:thebanana2)|(?:book)|(?:thebirthday)|(?:thecartman)|(?:theparty)|(?:cold)|(?:yinyang)|(?:elvis)|(?:kiss)|(?:thebanana)|(?:viking)|(?:fancy)|(?:flash)|(?:pirate)|(?:mrT)|(?:holmes)|(?:netlog)|(?:afro)|(?:thebounce)|(?:stinky)|(?:badboy)|(?:huh)|(?:cry2)|(?:evileyes)|(?:angel)|(?:music)|(?:razor)|(?:rants)|(?:scanner)|(?:rock)|(?:blink)|(?:flower)|(?:weirdo)|(?:wacko)|(?:sick)|(?:clown)|(?:ninja)|(?:southpark)|(?:thewave)|(?:fool)|(?:puzzled)|(?:doh)|(?:girly)|(?:innocent)|(?:inlove)|(?:celebrate)|(?:dohh)|(?:bow)|(?:cry)|(?:lol)|(?:theflash)|(?:wow)|(?:x)|(?:thumbsup)|(?:whistle)|(?:crazylove)|(?:wub)|(?:blush)|(?:hug)|(?:love)|(?:blush_shy)|(?:thumbs_down)|(?:blush_shy)|(?:thumbs_up))\\]')\n",
        "\t#Netlog_emoticons1 = re.compile(r'\\[(?:(?: Love )|(?: blush )|(?: dohh )|(?: hug )|(?: whistle )|(?: x )|(?:Crazylove)|(?:Flash)|(?:Hug)|(?:theband)|(?:thebonk)|(?:theflasher)|(?:theguitare)|(?:thehappyherman)|(?:theheadphone)|(?:unsure)|(?: girly )|(?: love  )|(?:LOVE)|(?:thejesus)|(?:thedancer)|(?:thebanana2)|(?:book)|(?:thebirthday)|(?:thecartman)|(?:theparty)|(?:cold)|(?:yinyang)|(?:elvis)|(?:kiss)|(?:thebanana)|(?:viking)|(?:fancy)|(?:flash)|(?:pirate)|(?:mrT)|(?:holmes)|(?:netlog)|(?:afro)|(?:thebounce)|(?:stinky)|(?:badboy)|(?:huh)|(?:cry2)|(?:evileyes)|(?:angel)|(?:music)|(?:razor)|(?:rants)|(?:scanner)|(?:rock)|(?:blink)|(?:flower)|(?:weirdo)|(?:wacko)|(?:sick)|(?:clown)|(?:ninja)|(?:southpark)|(?:thewave)|(?:fool)|(?:puzzled)|(?:doh)|(?:girly)|(?:innocent)|(?:inlove)|(?:celebrate)|(?:dohh)|(?:bow)|(?:cry)|(?:lol)|(?:theflash)|(?:wow)|(?:x)|(?:thumbsup)|(?:whistle)|(?:crazylove)|(?:wub)|(?:blush)|(?:hug)|(?:love)|(?:blush_shy)|(?:thumbs_down)|(?:blush_shy)|(?:thumbs_up))\\]')\n",
        "\t#Netlog_emoticons2 = re.compile(r'\\[@[a-zA-Z0-9\\-]+\\]')\n",
        "\t#Netlog_emoticons3 = re.compile(r'\\[#\\w+(?:_anim)?\\]')\n",
        "\t\n",
        "\t## Recovered emoticons ### i.e. when the annotators have recovered smileys that were first automatically deleted. they have placed them between square brackets\n",
        "\t# not relevant in this corpus\n",
        "\t#recovered_emoticons = re.compile(r'\\[((<?[:;xXB\\|=]\\'?[-^o]?[\\)\\(xXpPSsdDVoO\\|\\/\\*$\\[\\]])|([\\)\\(][-^o]?\\'?[:;]>?))\\]')\n",
        "\n",
        "\t### Facebook emoticons ### i.e. in plain text format: e.g. 'wink-emoticon'\n",
        "\tfb_emoticon_indicator = re.compile(r'\\w+-emoticon')\n",
        "\n",
        "\t### manual emoticon indicators ### i.e. emoticons that are transcribed with an indicator: 'XEMOTICONX'\n",
        "\t#emoticon_indicator = re.compile(r'XEMOTICONX(_[a-zA-Z_]+)?')\n",
        "\t#emoticon_indicator_detailed = re.compile(r'XEMOTICONX_[a-zA-Z_]+')\n",
        "\temoticon_indicator = re.compile(r'XEMOTICONX(?:_[a-zA-Z_]+)?')\n",
        "\temoticon_indicator_detailed = re.compile(r'XEMOTICONX_(?:[a-zA-Z_]+)')\n",
        "\n",
        "\t### L/J emoticons ### i.e. smileys that are automatically replaced by 'L' or 'J'\n",
        "\t# not relevant in this corpus\n",
        "\t#J_emoticon = re.compile(r'^J+$')\n",
        "\t#L_emoticon = re.compile(r'^L+$')\n",
        "\n",
        "\n",
        "\t# we will store the emoticons in a container\n",
        "\temoticons = defaultdict(int)\n",
        "\n",
        "\t# we will only look for emoticons in words that are no hyperlinks\n",
        "\ttokens_ = [token for token in tokens if not hyperlink.search(token)]\n",
        "\n",
        "\tfor item in tokens_:\n",
        "\t\t# we want to update the number of words in the text if one word/token contains multiple smileys\n",
        "\t\t# (to avoid that for instance the utterance ':):)' contains 200% emoticons (2 emots divided by 1 word))\n",
        "\t\tnr_emoticons_found = 0\n",
        "\n",
        "\t\t# we first want to restore the recovered emoticons by removing the square brackets\n",
        "\t\t\"\"\"if recovered_emoticons.search(item):\n",
        "\t\t\titem = item.replace('[','')\n",
        "\t\t\titem = item.replace(']','')\"\"\"\n",
        "\n",
        "\t\t# then we check for different kinds of emoticons\n",
        "\n",
        "\t\tif emoticon_indicator.search(item):\n",
        "\t\t\t# we do not want to miss any occurrences (in case spaces are ommitted and we have XEMOTICONXXEMOTICONX)\n",
        "\t\t\toccs = emoticon_indicator.findall(item)\n",
        "\t\t\t# we add the occurrences to the general counter for unknown emoticons, and count the individual variants\n",
        "\t\t\tfor occ in occs:\n",
        "\t\t\t\tif emoticon_indicator_detailed.search(occ):\n",
        "\t\t\t\t\tif 'face' in occ:\n",
        "\t\t\t\t\t\temoticons['Faces variants'] += 1\n",
        "\t\t\t\t\telif (('heart' in occ) or ('kiss' in occ)):\n",
        "\t\t\t\t\t\temoticons['Heart variants'] += 1\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\temoticons['Pictogram variants'] += 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\temoticons['Unknown variants'] += 1\n",
        "\t\t\t\t\n",
        "\t\t\t\temoticons[occ] += 1\n",
        "\t\t\t\t# we update the nr of emoticons found\n",
        "\t\t\t\tnr_emoticons_found += 1\n",
        "\n",
        "\t\t\"\"\"if (J_emoticon.search(item)):\n",
        "\t\t\tnr_occs = len(item) # we do not work with findall as this would give much noise for this emoticon\n",
        "\t\t\t# we add the occurrences to the general counter for western emoticons, and count the individual variants\n",
        "\t\t\temoticons['Faces'] += nr_occs\n",
        "\t\t\temoticons['J'] += nr_occs\n",
        "\t\t\t# we update the nr of emoticons found\n",
        "\t\t\tnr_emoticons_found += nr_occs\"\"\"\n",
        "\n",
        "\t\t\"\"\"if (L_emoticon.search(item)):\n",
        "\t\t\tnr_occs = len(item) # we do not work with findall as this would give much noise for this emoticon\n",
        "\t\t\t# we add the occurrences to the general counter for western emoticons, and count the individual variants\n",
        "\t\t\temoticons['Faces'] += nr_occs\n",
        "\t\t\temoticons['L'] += nr_occs\n",
        "\t\t\t# we update the nr of emoticons found\n",
        "\t\t\tnr_emoticons_found += nr_occs\"\"\"\n",
        "\n",
        "\n",
        "\t\tif western_emoticons_all.search(item):\n",
        "\t\t\t# we do not want to miss any occurrences (in case spaces are ommitted and we have hello:) )\n",
        "\t\t\toccs = western_emoticons_all.findall(item)\n",
        "\n",
        "\t\t\t# as the regex for Western emoticons contains | (OR), findall may result in a list of tuples instead of a list of strings\n",
        "\t\t\t# we therefore extract the elements from the tuples and save them in a new list\n",
        "\t\t\tnew_occs = []\n",
        "\t\t\tif len(occs) != 0:\n",
        "\t\t\t\t# if the list of occurrences containts a tuple, we want to extract the strings from these tuples\n",
        "\t\t\t\tif ((\"<class 'tuple'>\" in [str(type(element)) for element in occs]) or (\"<type 'tuple'>\" in [str(type(element)) for element in occs])):\n",
        "\t\t\t\t\tfor tuple_ in occs:\n",
        "\t\t\t\t\t\tfor element in tuple_:\n",
        "\t\t\t\t\t\t\tif not ((element == '') or (element.lower() == 'xs') or (not_western_emoticons.search(element)) or (not_western_emoticons.search(element.lower()))):\n",
        "\t\t\t\t\t\t\t\t# we do not count flooding of kisses as emoji (e.g. XX or XO)\n",
        "\t\t\t\t\t\t\t\tif not (flooding_x.search(element) or flooding_xo.search(element)):\n",
        "\t\t\t\t\t\t\t\t\tnew_occs.append(element)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# else, we do not need to change the list of occurrences\n",
        "\t\t\t\t\tnew_occs = occs\n",
        "\n",
        "\t\t\t# we add the occurrences to the general counter for Western emoticons, and count the individual variants\n",
        "\t\t\tfor new_occ in new_occs:\n",
        "\t\t\t\temoticons['Faces variants'] += 1\n",
        "\t\t\t\temoticons[new_occ] += 1\n",
        "\t\t\t\t# we update the nr of emoticons found\n",
        "\t\t\t\tnr_emoticons_found += 1\n",
        "\n",
        "\n",
        "\t\tif fb_emoticon_indicator.search(item):\n",
        "\t\t\t# we do not want to miss any occurrences (in case spaces are ommitted and we have hellogrin-emoticon)\n",
        "\t\t\toccs = fb_emoticon_indicator.findall(item)\n",
        "\t\t\tnr_occs = len(occs)\n",
        "\n",
        "\t\t\tif len(occs) != 0:\n",
        "\t\t\t\tfor occ in occs:\n",
        "\t\t\t\t\tmeaningful_part_name = occ.replace(\"-emoticon\", \"\")\n",
        "\t\t\t\t\tif ((meaningful_part_name in facebook_faces) or (\"face\" in meaningful_part_name)):\n",
        "\t\t\t\t\t\temoticons['Faces variants'] += 1\n",
        "\t\t\t\t\telif (('heart' in meaningful_part_name) or ('kiss' in meaningful_part_name)):\n",
        "\t\t\t\t\t\temoticons['Hearts variants'] += 1\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\temoticons['Pictogram variants'] += 1\n",
        "\n",
        "\t\t\t\t\temoticons[occ] += 1\n",
        "\n",
        "\n",
        "\t\tif asian_emoticons.search(item):\n",
        "\t\t\t# we do not want to miss any occurrences (in case spaces are ommitted and we have hello^^)\n",
        "\t\t\toccs = asian_emoticons.findall(item)\n",
        "\n",
        "\t\t\t# as the regex for Asian emoticons contains | (OR), findall may result in a list of tuples instead of a list of strings\n",
        "\t\t\t# we therefore extract the elements from the tuples and save them in a new list\n",
        "\t\t\tnew_occs = []\n",
        "\t\t\tif len(occs) != 0:\n",
        "\t\t\t\t# if the list of occurrences contains a tuple, we want to extract the strings from these tuples\n",
        "\t\t\t\tif \"<class 'tuple'>\" in [str(type(element)) for element in occs]:\n",
        "\t\t\t\t\tfor tuple_ in occs:\n",
        "\t\t\t\t\t\tfor element in tuple_:\n",
        "\t\t\t\t\t\t\tif element != '':\n",
        "\t\t\t\t\t\t\t\tnew_occs.append(element)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# else, we do not need to change the list of occurrences\n",
        "\t\t\t\t\tnew_occs = occs\n",
        "\n",
        "\t\t\t# we add the occurrences to the general counter for Asian emoticons, and count the individual variants\n",
        "\t\t\tfor new_occ in new_occs:\n",
        "\t\t\t\temoticons['Faces variants'] += 1\n",
        "\t\t\t\temoticons[new_occ] += 1\n",
        "\t\t\t\t# we update the nr of emoticons found\n",
        "\t\t\t\tnr_emoticons_found += 1\n",
        "\n",
        "\t\tif hearts.search(item):\n",
        "\t\t\t# we do not want to miss any occurrences (in case spaces are ommitted and we have hello<3)\n",
        "\t\t\toccs = hearts.findall(item)\n",
        "\t\t\t# as the regex for heart emoticons contains | (OR), findall may result in a list of tuples instead of a list of strings\n",
        "\t\t\t# we therefore extract the elements from the tuples and save them in a new list\n",
        "\t\t\tnew_occs = []\n",
        "\t\t\tif len(occs) != 0:\n",
        "\t\t\t\t# if the list of occurrences contains a tuple, we want to extract the strings from these tuples\n",
        "\t\t\t\tif \"<class 'tuple'>\" in [str(type(element)) for element in occs]:\n",
        "\t\t\t\t\tfor tuple_ in occs:\n",
        "\t\t\t\t\t\tfor element in tuple_:\n",
        "\t\t\t\t\t\t\tif element != '':\n",
        "\t\t\t\t\t\t\t\tnew_occs.append(element)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# else, we do not need to change the list of occurrences\n",
        "\t\t\t\t\tnew_occs = occs\n",
        "\n",
        "\t\t\t# we add the occurrences to the general counter for heart emoticons, and count the individual variants\n",
        "\t\t\tfor new_occ in new_occs:\n",
        "\t\t\t\temoticons['Heart variants'] += 1\n",
        "\t\t\t\temoticons[new_occ] += 1\n",
        "\t\t\t\t# we update the nr of emoticons found\n",
        "\t\t\t\tnr_emoticons_found += 1\n",
        "\n",
        "\t\t\"\"\"if MSN_emoticons.search(item):\n",
        "\t\t\t# we do not want to miss any occurrences (in case spaces are ommitted and we have 'hello(H)')\n",
        "\t\t\toccs = MSN_emoticons.findall(item)\n",
        "\t\t\t# as the regex for MSN emoticons contains | (OR), findall may result in a list of tuples instead of a list of strings\n",
        "\t\t\t# we therefore extract the elements from the tuples and save them in a new list\n",
        "\t\t\tnew_occs = []\n",
        "\t\t\tif len(occs) != 0:\n",
        "\t\t\t\t# if the list of occurrences contains a tuple, we want to extract the strings from these tuples\n",
        "\t\t\t\tif \"<class 'tuple'>\" in [str(type(element)) for element in occs]:\n",
        "\t\t\t\t\tfor tuple_ in occs:\n",
        "\t\t\t\t\t\tfor element in tuple_:\n",
        "\t\t\t\t\t\t\tif element != '':\n",
        "\t\t\t\t\t\t\t\tnew_occs.append(element)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# else, we do not need to change the list of occurrences\n",
        "\t\t\t\t\tnew_occs = occs\n",
        "\n",
        "\t\t\t# we add the occurrences to the general counter for MSN emoticons, and count the individual variants\n",
        "\t\t\tfor new_occ in new_occs:\n",
        "\t\t\t\temoticons['MSN variants'] += 1\n",
        "\t\t\t\temoticons[new_occ] += 1\n",
        "\t\t\t\t# we update the nr of emoticons found\n",
        "\t\t\t\tnr_emoticons_found += 1\"\"\"\n",
        "\n",
        "\t\t\"\"\"if (Netlog_emoticons1.search(item) or Netlog_emoticons2.search(item) or Netlog_emoticons3.search(item)):\n",
        "\t\t\t# we do not want to miss any occurrences (in case spaces are ommitted and we have 'hey[ Love ]')\n",
        "\t\t\toccs = Netlog_emoticons1.findall(item) + Netlog_emoticons2.findall(item) + Netlog_emoticons3.findall(item)\n",
        "\t\t\t# as the regex for Netlog emoticons contains | (OR), findall may result in a list of tuples instead of a list of strings\n",
        "\t\t\t# we therefore extract the elements from the tuples and save them in a new list\n",
        "\t\t\tnew_occs = []\n",
        "\t\t\tif len(occs) != 0:\n",
        "\t\t\t\t# if the list of occurrences contains a tuple, we want to extract the strings from these tuples\n",
        "\t\t\t\tif \"<class 'tuple'>\" in [str(type(element)) for element in occs]:\n",
        "\t\t\t\t\tfor tuple_ in occs:\n",
        "\t\t\t\t\t\tfor element in tuple_:\n",
        "\t\t\t\t\t\t\tif element != '':\n",
        "\t\t\t\t\t\t\t\tnew_occs.append(element)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# else, we do not need to change the list of occurrences\n",
        "\t\t\t\t\tnew_occs = occs\n",
        "\n",
        "\t\t\t# we add the occurrences to the general counter for MSN emoticons, and count the individual variants\n",
        "\t\t\tfor new_occ in new_occs:\n",
        "\t\t\t\temoticons['Netlog variants'] += 1\n",
        "\t\t\t\temoticons[new_occ] += 1\n",
        "\t\t\t\t# we update the nr of emoticons found\n",
        "\t\t\t\tnr_emoticons_found += 1\"\"\"\n",
        "\n",
        "\t\t# we also count unicode emoji, which occur, for example, in WhatsApp\n",
        "\t\t# we loop over the characters of the word/token\n",
        "\t\t# we have to decode the item to use unicode.name \t# LH no: not in python 3\n",
        "\t\tfor char in item:#.decode('utf-8'):\n",
        "\t\t\t# we get the unicode name of the character\n",
        "\t\t\t# however: some control characters do not have a name - we take this scenario into account\n",
        "\t\t\ttry:\n",
        "\t\t\t\tcharname = uni.name(char)\n",
        "\t\t\texcept ValueError:\n",
        "\t\t\t\tcharname = None\n",
        "\n",
        "\n",
        "\t\t\t# per default, we assume that this character is a possible unicode emoji\n",
        "\t\t\tpossible_emoji = True\n",
        "\n",
        "\t\t\t# if however, the name or a part of its name is in our list of regular unicode characters, it is no emoji\n",
        "\t\t\t# if it is 'None', it is a control character, and also no emoji\n",
        "\t\t\tif ((charname in normal_unicode) or (charname == None)):\n",
        "\t\t\t\tpossible_emoji = False\n",
        "\t\t\telse:\n",
        "\t\t\t\t# this name often consists of multiple parts (e.g. 'LATIN SMALL LETTER')\n",
        "\t\t\t\t# we split it in different parts\n",
        "\t\t\t\tname_parts = charname.split()\n",
        "\t\t\t\tfor part in name_parts:\n",
        "\t\t\t\t\tif part in normal_unicode:\n",
        "\t\t\t\t\t\t# then we change the boolean parameter to False: no emoji\n",
        "\t\t\t\t\t\tpossible_emoji = False\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t# if an emoji is found, we add it to our freq dict and increase the counters\n",
        "\t\t\tif possible_emoji == True:\n",
        "\t\t\t\tif 'face' in charname.lower():\n",
        "\t\t\t\t\temoticons['Faces variants'] += 1\n",
        "\t\t\t\telif (('heart' in charname.lower()) or ('kiss' in charname.lower())):\n",
        "\t\t\t\t\temoticons['Hearts variants'] += 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\temoticons['Pictogram variants'] += 1\n",
        "\n",
        "\t\t\t\t### LLL aangepast! emoticons[charname] += 1\n",
        "\t\t\t\temoticons[char] += 1\n",
        "\t\t\t\t# we update the nr of emoticons found\n",
        "\t\t\t\tnr_emoticons_found += 1\n",
        "\n",
        "\t\t# if necessary, we update the number of tokens in the text (i.e. if a token consists of multiple smileys, e.g. ':D:D')\n",
        "\t\tlength_tokens = len(tokens) + max([0,(nr_emoticons_found - 1)])\n",
        "\n",
        "\t# percentage emoticons in the text: nr. of emoticons divided by number of words (including emoticons)\n",
        "\t# we will count all emoticons that occur, but we exclude the general counters (so as not to count each emoticon twice)\n",
        "\tnr_emoticons = sum([emoticons[key] for key in emoticons if not key.endswith('variants')])\n",
        "\tif len(tokens) != 0:\n",
        "\t\tpercentage_emoticons = float(nr_emoticons)/len(tokens)\n",
        "\telse:\n",
        "\t\tpercentage_emoticons = 0\n",
        "\n",
        "\t# we return these numbers and the freq dict\n",
        "\treturn(nr_emoticons,percentage_emoticons,emoticons)\n",
        "\n",
        "\n",
        "##### FEATURE: UNCONVENTIONAL CAPITALIZATION #####\n",
        "\n",
        "def unconv_capitalization(tokens):\n",
        "\t\"\"\"\n",
        "\tthis function analyzes unconventional capitalization in a text\n",
        "\n",
        "\tinput:\n",
        "\t\t- 'tokens': a text split on whitespaces (i.e. a list of the tokens of a text)\n",
        "\n",
        "\toutput:\n",
        "\t\t- the absolute number of unconv. capitalized words in the text\n",
        "\t\t- the relative number of unconv. capitalized words in the text\n",
        "\t\t- a freq dict of the unconv. capitalized words\n",
        "\t\"\"\"\n",
        "\n",
        "\t# a regex we will need later on\n",
        "\tpunc = re.compile(r'[\\!\\?\\.;\\:,\\/\\-\\(\\)\\'\\\"\\-]+')\n",
        "\n",
        "\t# we create a container to store the unconv. capitalized words\n",
        "\tunconv_caps = defaultdict(int)\n",
        "\n",
        "\t# we define different kinds of unconventional capitalization\n",
        "\tall_caps = re.compile(r'^[A-Z]{2,}$') # allcaps: (HELLO)\n",
        "\tinverse_caps = re.compile(r'^[a-z][A-Z]+$') # inverse caps: (hELLO)\n",
        "\talternating_caps = re.compile(r'(^([a-z][A-Z])+[a-z]?$)|(^([A-Z][a-z])+[A-Z]?$)') # alternating caps: (HeLlO or hElLo)\n",
        "\t\n",
        "\n",
        "\t# we do not want to count certain emoticons as allcaps, e.g. 'XD'\n",
        "\twestern_emoticons_all = re.compile(r'(<?[:;xXB\\|=]\\'?[-^o]?[\\)\\(xXpPSsdDVoO\\|\\/\\*$\\[\\]])|([\\)\\(][-^o]?\\'?[:;]>?)')\n",
        "\n",
        "\t# we count the number of occurrences of unconventional caps\n",
        "\tfor token in tokens:\n",
        "\t\t# we only want to store the actual word, no punct marks attached to it\n",
        "\t\tonly_word = punc.sub('',token)\n",
        "\t\tonly_word_lower = only_word.lower()\n",
        "\t\t# if what is left is no empty string, we check if the remaining letters match the pattern\n",
        "\t\tif all_caps.search(only_word):\n",
        "\t\t\t# we do not count emoticons (e.g. XD), name indicators nor emoticon indicators as allcaps\n",
        "\t\t\tif not (western_emoticons_all.search(only_word) or (only_word_lower.startswith('xemoticonx')) or (only_word_lower == 'xnaamx')):\n",
        "\t\t\t\t# if the word is a standard Dutch abbreviation, it is okay to write it in caps\n",
        "\t\t\t\tif not only_word_lower in abbrev_dict:\n",
        "\t\t\t\t\tunconv_caps[only_word] += 1\n",
        "\t\t\t\t\t# we also increase a general counter for allcaps\n",
        "\t\t\t\t\tunconv_caps['allcaps variants'] += 1\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\tif (not_western_emoticons.search(only_word_lower) or flooding_x.search(only_word_lower) or flooding_xo.search(only_word_lower)):\n",
        "\t\t\t\t\tif not (only_word_lower.startswith('xemoticonx') or (only_word_lower == 'xnaamx')):\n",
        "\t\t\t\t\t\tunconv_caps[only_word] += 1\n",
        "\t\t\t\t\t\tunconv_caps['allcaps variants'] += 1\n",
        "\n",
        "\t\telif (inverse_caps.search(only_word) and (len(only_word) > 2)):\n",
        "\t\t\t# we do not count emoticons (e.g. xP) as inverse caps\n",
        "\t\t\tif not western_emoticons_all.search(only_word):\n",
        "\t\t\t\tunconv_caps[only_word] += 1\n",
        "\t\t\t\t# we also increase a general counter for inverse caps\n",
        "\t\t\t\tunconv_caps['inverse caps variants'] += 1\n",
        "\n",
        "\t\t\telse:\n",
        "\n",
        "\t\t\t\tif (not_western_emoticons.search(only_word_lower) or flooding_x.search(only_word_lower) or flooding_xo.search(only_word_lower)):\n",
        "\t\t\t\t\tunconv_caps[only_word] += 1\n",
        "\t\t\t\t\tunconv_caps['inverse caps variants'] += 1\n",
        "\n",
        "\t\telif (alternating_caps.search(only_word) and (len(only_word) > 2)):\n",
        "\t\t\t# we do not count emoticons (e.g. xP) as inverse caps\n",
        "\t\t\t# we also do not count words with only two letters as inverse caps, as this is standard capitalization (eg 'En ...', 'Ik ...')\n",
        "\t\t\tif not western_emoticons_all.search(only_word):\n",
        "\t\t\t\tunconv_caps[only_word] += 1\n",
        "\t\t\t\t# we also increase a general counter for alternating caps\n",
        "\t\t\t\tunconv_caps['alternating caps variants'] += 1\n",
        "\n",
        "\t\t\telse:\n",
        "\n",
        "\t\t\t\tif (not_western_emoticons.search(only_word_lower) or flooding_x.search(only_word_lower) or flooding_xo.search(only_word_lower)):\n",
        "\t\t\t\t\tunconv_caps[only_word] += 1\n",
        "\t\t\t\t\tunconv_caps['alternating caps variants'] += 1\n",
        "\n",
        "\n",
        "\t# we calculate abs and rel numbers\n",
        "\tnr_unconvcaps = sum([unconv_caps[key] for key in unconv_caps if not key.endswith('variants')])\n",
        "\n",
        "\tif len(tokens) != 0:\n",
        "\t\tpercentage_unconvcaps = float(nr_unconvcaps)/len(tokens)\n",
        "\telse:\n",
        "\t\tpercentage_unconvcaps = 0\n",
        "\n",
        "\t# we return the absolute and relative nr as well as the freq dict of unconv. capitalized words\n",
        "\treturn(nr_unconvcaps,percentage_unconvcaps,unconv_caps)\n",
        "\n",
        "\n",
        "##### FEATURE: KISSES AND HUGS #####\n",
        "\n",
        "def kisses_and_hugs(tokens_lower):\n",
        "\t\"\"\"\n",
        "\tthis function analyzes the use of the letter(s) 'x' (and 'o') representing kisses (and hugs)\n",
        "\n",
        "\tinput:\n",
        "\t\t- 'tokens_lower': a lowercased text split on whitespaces (i.e. a list of the tokens of a lowercased text)\n",
        "\n",
        "\toutput:\n",
        "\t\t- the absolute number of (groups of) kisses, expressed by the letter(s) 'x' (and 'o')\n",
        "\t\t- the relative number of (groups of) kisses, expressed by the letter(s) 'x' (and 'o')\n",
        "\t\t- a freq dict of these occurrences\n",
        "\t\"\"\"\n",
        "\n",
        "\t# we create a container to store the kisses and hugs\n",
        "\tkisses_dict = defaultdict(int)\n",
        "\n",
        "\t# we define these kisses\n",
        "\t# kisses: one or more times the letter 'x', isolated\n",
        "\tkisses = re.compile(r'^x+$')\n",
        "\t# kisses and hugs: one or more times the combination 'xo', isolated\n",
        "\tkisses_hugs = re.compile(r'^(x+o+)+x?$')\n",
        "\n",
        "\t# we store the occurrences of kisses and hugs\n",
        "\tfor token in tokens_lower:\n",
        "\t\tif kisses.search(token):\n",
        "\t\t\t# we add 1 to the general counter for all x-variants\n",
        "\t\t\tkisses_dict['x-variants'] += 1\n",
        "\t\t\t# we also count the individual variant\n",
        "\t\t\tkisses_dict[token] += 1\n",
        "\n",
        "\t\telif kisses_hugs.search(token):\n",
        "\t\t\t# we add 1 to the general counter for all xoxo-variants\n",
        "\t\t\tkisses_dict['xoxo-variants'] += 1\n",
        "\t\t\t# we also count the individual variant\n",
        "\t\t\tkisses_dict[token] += 1\n",
        "\n",
        "\t# we count the occurrences of kisses and hugs\n",
        "\tdont_include = ['x-variants','xoxo-variants']\n",
        "\tnr_kisses = sum([kisses_dict[key] for key in kisses_dict if key not in dont_include])\n",
        "\t# the percentage of kisses and hugs:\n",
        "\tif len(tokens_lower) != 0:\n",
        "\t\tpercentage_kisses = float(nr_kisses)/len(tokens_lower)\n",
        "\telse:\n",
        "\t\tpercentage_kisses = 0\n",
        "\n",
        "\t# we return this relative number and the freq dict\n",
        "\treturn(nr_kisses,percentage_kisses,kisses_dict)\n",
        "\n",
        "\n",
        "\n",
        "##### FEATURE: UNCONVENTIONAL PUNCTUATION #####\n",
        "\n",
        "# remark: the analysis of ALL kinds of unconventional punct. would lead to a considerable overlap with the detected emoticons and punctuation flooding\n",
        "# which is why, in this function, we will only analyze combinations of ? and ! (e.g. 'hello?!?!')\n",
        "\n",
        "def unconv_combis(tokens):\n",
        "\t\"\"\"\n",
        "\tthis function analyzes the unconventional combinations of question and exclamation marks\n",
        "\n",
        "\tinput:\n",
        "\t\t- 'tokens': a text split on whitespaces (i.e. a list of the tokens of a text)\n",
        "\n",
        "\toutput:\n",
        "\t\t- the relative number of unconventional combinations of question and excl. marks\n",
        "\t\t- a freq.dict. of these occurrences\n",
        "\t\"\"\"\n",
        "\n",
        "\t# we create a container for the unconv. combinations\n",
        "\tunconv_combis_dict = defaultdict(int)\n",
        "\n",
        "\t# we define the unconventional combinations\n",
        "\tunconv_combis_regex = re.compile(r'(![!?]*\\?[!?]*)|(\\?[!?]*![!?]*)')\n",
        "\n",
        "\t# we only want to store the punctuation marks, and not the letters attached\n",
        "\tclean_punc = []\n",
        "\n",
        "\tfor token in tokens:\n",
        "\t\tclean = [item for item in re.compile(r'\\w+').split(token) if item != \"\"]\n",
        "\t\tclean_punc += clean\n",
        "\n",
        "\t# we store the unconventional combi's in a list\n",
        "\tfor token in clean_punc:\n",
        "\t\tif unconv_combis_regex.search(token):\n",
        "\t\t# we store the occurrence\n",
        "\t\t\tunconv_combis_dict[token] += 1\n",
        "\n",
        "\t# we count all other occurrences of ! and ?\n",
        "\t# we initiate a counter at zero\n",
        "\tother_occurrences = 0\n",
        "\tsingle_punct = re.compile(r'^[\\?!]$')\n",
        "\tflooded_punct = re.compile(r'^(\\?|!)\\1{1,}$') # isolated flooded punct\n",
        "\n",
        "\tfor token in clean_punc:\n",
        "\t\tif (single_punct.match(token) or flooded_punct.match(token)):\n",
        "\t\t\tother_occurrences += 1\n",
        "\n",
        "\t# we count all occurrences of ? and !\n",
        "\tnr_unconv_combis = sum(unconv_combis_dict.values())\n",
        "\ttotal_occ = other_occurrences + nr_unconv_combis\n",
        "\n",
        "\t# the percentage of unconventional combis: the nr of these combi's divided by all occurrences of ? and !\n",
        "\tif total_occ != 0:\n",
        "\t\tpercentage_unconv_combis = float(nr_unconv_combis)/total_occ\n",
        "\telse:\n",
        "\t\tpercentage_unconv_combis = 0\n",
        "\n",
        "\t# we return this percentage and the freq dict of instances\n",
        "\treturn(nr_unconv_combis,percentage_unconv_combis,unconv_combis_dict)\n",
        "\n",
        "\n",
        "##### FEATURE: LEETSPEAK #####\n",
        "\n",
        "\n",
        "\n",
        "# we define a list of tokens that must not be confused for leetspeak\n",
        "not_leetspeak = ['9gag', 'ps3', 'ps4', 'ps5', 'ps1', 'ps2', 'a1', 'a2', 'a3', 'a4', 'a5', 'c4', 'k3', 'mp3', 'mp4', 'a12', 'e19', 'e17', 'e40', 'e313']\n",
        "\n",
        "letters = re.compile(r'[a-z]|[A-Z]+')\n",
        "leetspeak_signs = re.compile(r'[0-9@\\$€£]+')\n",
        "\n",
        "\n",
        "def leetspeak(tokens):\n",
        "\t\"\"\"\n",
        "\tthis function analyzes the use of leetspeak in a text\n",
        "\n",
        "\tinput: \n",
        "\t\t- 'tokens': a text split on whitespaces (i.e. a list of the tokens of a text)\n",
        "\n",
        "\toutput:\n",
        "\t\t- the abs and relative number of leetspeak in the text\n",
        "\t\t- a freq dict of the words containing leetspeak\n",
        "\t\"\"\"\n",
        "\n",
        "\n",
        "\t# we create a container for the leetspeak occurrences\n",
        "\tleetspeak_words = defaultdict(int)\n",
        "\n",
        "\t# we count the occurrences of leetspeak\n",
        "\t# leetspeak: a word written as a combi of letters and numbers (e.g. 'w8' = 'wait') or letters and special signs (e.g. 'wh@t' = \"what\")\n",
        "\tfor token in tokens:\n",
        "\t\ttoken = token.lower()\n",
        "\t\tif letters.search(token) and leetspeak_signs.search(token):\n",
        "\t\t\t# we exclude some exceptions: leetspeak forms that are actually standard Dutch\n",
        "\t\t\tif not (token in not_leetspeak):\n",
        "\t\t\t\t# we do not want to count hour references as leetspeak, or page numbers, ...\n",
        "\t\t\t\tif not (hour_refs.search(token) or meter.search(token) or pages.search(token) or money.search(token) or files.search(token) or numbers_and_ordinals.search(token) or ordinals_EN.search(token) or hyperlink.search(token) or email.search(token) or ats.search(token) or (emoticons([token])[0] > 0)):\n",
        "\t\n",
        "\t\t\t\t\tleetspeak_words[token] += 1\n",
        "\n",
        "\n",
        "\t# percentage leetspeak: words containing leetspeak divided by all words\n",
        "\tnr_leetspeak = sum(leetspeak_words.values())\n",
        "\tif len(tokens) > 0:\n",
        "\t\tpercentage_leetspeak = float(nr_leetspeak)/len(tokens)\n",
        "\telse:\n",
        "\t\tpercentage_leetspeak = 0\n",
        "\n",
        "\t# we return the abs and rel frequency and the occurrences\n",
        "\treturn(nr_leetspeak,percentage_leetspeak,leetspeak_words)\n",
        "\n",
        "##### FEATURE: CHATSPEAK ABBREVIATIONS AND ACRONYMS ######\n",
        "\n",
        "# we store popular* chatspeak abbreviations and acronyms in a dictionary, along with their original (longer) word(s)\n",
        "# *popular among Flemish teenagers\n",
        "\n",
        "\n",
        " \n",
        "def abbrev(clean_tokens_lower):\n",
        "\t\"\"\"\n",
        "\tthis function analyzes the use of typical chatspeak (i.e. NON-standard) abbreviations and acronyms in a text\n",
        "\n",
        "\tinput:\n",
        "\t\t- 'clean_tokens_lower': a lowercased text split on whitespaces and punctuation marks\n",
        "\n",
        "\toutput:\n",
        "\t\t- the absolute and relative number of abbrev. and acronyms in the text\n",
        "\t\t- a freq. dict of the abbrev. and acronyms\n",
        "\t\"\"\"\n",
        "\n",
        "\t# we create a container for the abbrev. and acronyms\n",
        "\tabbrev_forms = defaultdict(int)\n",
        "\n",
        "\t# we check if the words are abbreviations or acronyms\n",
        "\tfor token in clean_tokens_lower:\n",
        "\n",
        "\t\t# abbrev\n",
        "\t\tfor key in abbrev_dict:\n",
        "\t\t\tval = abbrev_dict[key]\n",
        "\t\t\t# the value of the original word (key) can be one abbrev, or a list of abbrevs\n",
        "\t\t\tif (((type(val) == str) and (token == val)) or ((type(val) == list) and (token in val))):\n",
        "\t\t\t#if ((word == val) or (word in val)):\n",
        "\t\t\t\tfull_word = key\n",
        "\t\t\t\t# we add the abbreviation to the dictionary\n",
        "\t\t\t\tabbrev_forms[token] += 1\n",
        "\t\t\t\t# we also increase the counter for the abbreviations\n",
        "\t\t\t\tabbrev_forms['abbreviations variants'] += 1\n",
        "\t\t\t\t# finally, we store the full word\n",
        "\t\t\t\tname_fullword_key = full_word + ' variants'\n",
        "\t\t\t\tabbrev_forms[name_fullword_key] += 1\n",
        "\n",
        "\t\t\t\t# if we have found the full word, we leave the for-loop\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t# acronyms\n",
        "\t\tfor key in acronym_dict_EN:\n",
        "\t\t\tval = acronym_dict_EN[key]\n",
        "\t\t\t# the value of the original word (key) can be one acronym, or a list of acronyms\n",
        "\t\t\tif (((type(val) == str) and (token == val)) or ((type(val) == list) and (token in val))):\n",
        "\t\t\t#if ((word == val) or (word in val)):\n",
        "\t\t\t\tfull_phrase = key\n",
        "\t\t\t\t# we add the acronym to the dictionary\n",
        "\t\t\t\tabbrev_forms[token] += 1\n",
        "\t\t\t\t# we also increase the counter for the acronyms\n",
        "\t\t\t\tabbrev_forms['acronyms variants'] += 1\n",
        "\t\t\t\t# finally, we store the full phrase\n",
        "\t\t\t\tname_fullphrase_key = full_phrase + ' variants'\n",
        "\t\t\t\tabbrev_forms[name_fullphrase_key] += 1\n",
        "\n",
        "\t\t\t\t# if we have found the full phrase, we leave the for-loop\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t# acronyms\n",
        "\t\tfor key in acronym_dict_IT:\n",
        "\t\t\tval = acronym_dict_IT[key]\n",
        "\t\t\t# the value of the original word (key) can be one acronym, or a list of acronyms\n",
        "\t\t\tif (((type(val) == str) and (token == val)) or ((type(val) == list) and (token in val))):\n",
        "\t\t\t#if ((word == val) or (word in val)):\n",
        "\t\t\t\tfull_phrase = key\n",
        "\t\t\t\t# we add the acronym to the dictionary\n",
        "\t\t\t\tabbrev_forms[token] += 1\n",
        "\t\t\t\t# we also increase the counter for the acronyms\n",
        "\t\t\t\tabbrev_forms['acronyms variants'] += 1\n",
        "\t\t\t\t# finally, we store the full phrase\n",
        "\t\t\t\tname_fullphrase_key = full_phrase + ' variants'\n",
        "\t\t\t\tabbrev_forms[name_fullphrase_key] += 1\n",
        "\n",
        "\t\t\t\t# if we have found the full phrase, we leave the for-loop\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t# we calculate the abs and rel numbers\n",
        "\tnr_abbrev_forms = sum([abbrev_forms[key] for key in abbrev_forms if not key.endswith('variants')])\n",
        "\n",
        "\tif len(clean_tokens_lower) != 0:\n",
        "\t\tpercentage_abbrev_forms = float(nr_abbrev_forms)/len(clean_tokens_lower)\n",
        "\telse:\n",
        "\t\tpercentage_abbrev_forms = 0\n",
        "\n",
        "\t# we return the abs and rel number as well as the list of occurrences\n",
        "\treturn(nr_abbrev_forms,percentage_abbrev_forms,abbrev_forms)\n",
        "\n",
        "\n",
        "\n",
        "##### FEATURE: LAUGHTER #####\n",
        "\n",
        "def laughter(tokens_lower):\n",
        "    \"\"\"\n",
        "    this function analyzes laughter in a text, expressed by 'haha', 'hihi', etc.\n",
        "\n",
        "    input:\n",
        "        - 'tokens_lower': a lowercased text split on whitespaces (i.e. a list of the tokens of a lowercased text)\n",
        "    \n",
        "    output: \n",
        "        - the absolute frequency of laughter in the texet, expressed by 'haha', 'hihi', etc.\n",
        "        - the relative frequency of laughter in the texet, expressed by 'haha', 'hihi', etc.\n",
        "        - a freq dict of these occurrences\n",
        "    \"\"\"\n",
        "\n",
        "    # we create containers to store the laughter\n",
        "    laughter_dict = defaultdict(int)\n",
        "\n",
        "    # we define laughter\n",
        "    # haha variants: one or more times 'haha', + variants like haaahhaa\n",
        "    haha = re.compile(r'w?(h+a+){2,}h?')\n",
        "    # hihi variants: one or more times 'hihi', + variants like hiihi\n",
        "    hihi = re.compile(r'(h+i+){2,}h?')\n",
        "\t# ahah variants: one or more times 'haha', + variants like haaahhaa\n",
        "    ahah = re.compile(r'w?(a+h+){2,}h?')\n",
        "\n",
        "    # a regex we will need later on: punctuation\n",
        "    punc = re.compile(r'[\\!\\?\\.;:,\\/\\-\\(\\)\\'\\\"\\-]+')\n",
        "\t\n",
        "    # we store the occurrences of laughter\n",
        "    for token in tokens_lower:\n",
        "\t\t\n",
        "        if haha.search(token):\n",
        "            # we add 1 to the general counter for haha-variants\n",
        "            laughter_dict['haha-variants'] += 1\n",
        "            # we also count the individual variant\n",
        "            # we only want to store the word itself, and no punctuation marks attached to it\n",
        "            only_word = punc.sub('',token)\n",
        "            laughter_dict[only_word] += 1\n",
        "\n",
        "        if hihi.search(token):\n",
        "            # we add 1 to the general counter for hihi-variants\n",
        "            laughter_dict['hihi-variants'] += 1\n",
        "            # we also count the individual variant\n",
        "            # we only want to store the word itself, and no punctuation marks attached to it\n",
        "            only_word = punc.sub('',token)\n",
        "            laughter_dict[only_word] += 1\n",
        "\t\t\n",
        "        if ahah.search(token):\n",
        "\t\t\t# we add 1 to the general counter for hihi-variants\n",
        "            laughter_dict['ahah-variants'] += 1\n",
        "            # we also count the individual variant\n",
        "            # we only want to store the word itself, and no punctuation marks attached to it\n",
        "            only_word = punc.sub('',token)\n",
        "            laughter_dict[only_word] += 1\n",
        "\n",
        "    # we count the occurrences of laughter\n",
        "    dont_include = ['haha-variants','hihi-variants', 'ahah-variants']\n",
        "    nr_laughter = sum([laughter_dict[key] for key in laughter_dict if key not in dont_include])\n",
        "    # the percentage of laughter:\n",
        "    if len(tokens_lower) != 0:\n",
        "        percentage_laughter = float(nr_laughter)/len(tokens_lower)\n",
        "    else:\n",
        "        percentage_laughter = 0\n",
        "\n",
        "    # we return this relative number and the freq dict\n",
        "    return(nr_laughter,percentage_laughter,laughter_dict)\n",
        "\n",
        "\n",
        "##### FEATURE: DISCOURSE MARKER #####\n",
        "\n",
        "def discourse(tokens):\n",
        "\t\"\"\"\n",
        "\tdetects Twitter-related discourse markers in a text (hashtags and @ )\n",
        "\n",
        "\tinput: \n",
        "\t\t- 'tokens': a text split on whitespaces (i.e. a list of the tokens of a text)\n",
        "\n",
        "\toutput: \n",
        "\t\t- the abs and rel numbers as well as the freq dict of:\n",
        "\t\t\t- hashtags\n",
        "\t\t\t- @ + name\n",
        "\t\"\"\"\n",
        "\t# we create a container for the occurrences\n",
        "\tdiscourse_markers = defaultdict(int)\n",
        "\n",
        "\t# we define the different elements of discourse markers\n",
        "\thashtags = re.compile(r'^#\\w+$')\n",
        "\tats = re.compile(r'^@[a-zA-Z]+:?$')\n",
        "\n",
        "\n",
        "\t# we count the occurrences\n",
        "\tfor token in tokens:\n",
        "\n",
        "\t\ttoken_lower = token.lower()\n",
        "\n",
        "\t\tif hashtags.search(token):\n",
        "\t\t\tdiscourse_markers[token_lower] += 1\n",
        "\t\t\tdiscourse_markers['hashtag variants'] += 1\n",
        "\n",
        "\t\telif ats.search(token):\n",
        "\t\t\tdiscourse_markers[token_lower] += 1\n",
        "\t\t\tdiscourse_markers['ats variants'] += 1\n",
        "\n",
        "\t# percentage: occurrences divided by all tokens\n",
        "\tnr_discourse_markers = sum([discourse_markers[key] for key in discourse_markers if not key.endswith('variants')])\n",
        "\n",
        "\tif len(tokens) > 0:\n",
        "\t\tpercentage_discourse_markers = float(nr_discourse_markers)/len(tokens)\n",
        "\telse:\n",
        "\t\tpercentage_discourse_markers = 0\n",
        "\n",
        "\t# we return the abs and rel frequency and the occurrences\n",
        "\treturn(nr_discourse_markers,percentage_discourse_markers,discourse_markers)\n",
        " \n",
        " \n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def italian_tokenizer(text: str) -> list:\n",
        "\n",
        "        \"\"\"A class method to tokenize Italian text. It returns the tokenized text.\n",
        "        \n",
        "        A class method to be run on the comments through apply to tokenize them.\n",
        "        The function is ideantical to the tokenizer above. However, it is meant to be\n",
        "        used for Italian data: nltk tokenizer does not split on the \"'\" correctly for Italian.\n",
        "    \n",
        "        Args:\n",
        "            self: reference to the current instance of the class\n",
        "            text (str): Any string\n",
        "\n",
        "        Returns:\n",
        "            txt (list): A list of strings. The tokenized input text.\n",
        "        \n",
        "        \"\"\"\n",
        "        try:\n",
        "\n",
        "            txt = word_tokenize(text, language='italian')\n",
        "            txt = [token for token in txt if token]\n",
        "            txt = [token for word in txt for token in word.split(\"'\")]\n",
        "\n",
        "        except LookupError:\n",
        "            \n",
        "            x = input('Model \"punkt\" is not installed yet, do you want to install it? Y | N')\n",
        "            if x.lower() == 'y':\n",
        "\n",
        "                    print('Downloading model...')\n",
        "                    nltk.download('punkt')\n",
        "                    print('Downloaded!')\n",
        "                    txt = word_tokenize(text, language='italian')\n",
        "                    txt = [token for token in txt if token]\n",
        "                    txt = [token for word in txt for token in word.split(\"'\")]\n",
        "            \n",
        "            else:\n",
        "                    txt = ModuleNotFoundError\n",
        "                    print('Please download \"punkt\" to used cometaNLP tokenizer')\n",
        "\n",
        "        return txt\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def preprocessor(txt: str) -> str:\n",
        "\n",
        "        \"\"\"A function to be run on the comments through apply to clean them.\n",
        "        \n",
        "        The function applies a series of transformation to the comments. Hashtags,\n",
        "        urls, and user tags are removed.Digits and leading/trailing spaces are also removed.\n",
        "\n",
        "        Args:\n",
        "            text (str): Any string\n",
        "        \n",
        "        Returns:\n",
        "            txt (str): The input text without hashtags, urls, etc.\n",
        "        \n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "    \n",
        "        #Noise removal based on the explain weights function of the baseline logistic regression\n",
        "        #txt = text.replace('url', '')\n",
        "        #txt = txt.replace('URL', '')\n",
        "\t\t\t\t\n",
        "        \n",
        "        \n",
        "\t\t\t\t#Hashtag substitute\n",
        "        txt = re.sub(\"#[A-Za-z0-9_]+\",\"HASH\", txt)\n",
        "    \n",
        "        #Genral user tag substitute (accounting also for potential differently anonymized data) remover\n",
        "        txt = re.sub(\"@[A-Za-z0-9_]+\",\"HASH\", txt)\n",
        "    \n",
        "        #Genral url substitute (same as above)\n",
        "        txt = re.sub(r\"http\\S+\", \"URL\", txt)\n",
        "        txt = re.sub(r\"www.\\S+\", \"URL\", txt)\n",
        "\t\n",
        "        # remove numbers\n",
        "        #txt = re.sub(r'\\d+', '', txt)\n",
        "        txt = txt.lower()\n",
        "\t\t\n",
        "        txt = \"\".join([char if char not in '!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~' else ' ' for char in txt]) \n",
        "\n",
        "        # Also, removes leading and trailing whitespaces\n",
        "        txt = re.sub('\\s+', ' ', txt).strip()\n",
        "\n",
        "        return txt\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def demojizer(text: str) -> str:\n",
        "\n",
        "        \"\"\"A function to be run on comments. It returns the number of urls.\n",
        "\n",
        "        Args:\n",
        "            text (str): Any string\n",
        "\n",
        "        Returns:\n",
        "            txt (str): The input text without emojis\n",
        "\n",
        "        \"\"\"\n",
        "        txt = emoji.replace_emoji(string=text, replace=' EMOJI ')\n",
        "        \n",
        "        return txt\n",
        "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------        \n",
        "def comment_length(l: list) -> int:\n",
        "        \n",
        "        \n",
        "        \"\"\"A function to be run on comments. It returns the length of the comments.\n",
        "\n",
        "        Args:\n",
        "            text (str): Any string\n",
        "\n",
        "        Returns:\n",
        "            count (int): The comment length\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        count = len(l)\n",
        "        \n",
        "        return count\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def word_counts(l: list) -> dict:\n",
        "\n",
        "        \n",
        "        \"\"\"A function to be run on comments. It returns a dictionary containing the word counts.\n",
        "\n",
        "        Args:\n",
        "            l (list): Any list of strings\n",
        "\n",
        "        Returns:\n",
        "            counts (dict): A word-counts dictionary\n",
        "\n",
        "        \"\"\"\n",
        "        counts = Counter()\n",
        "        \n",
        "        for token in l:\n",
        "            counts[token] += 1\n",
        "        \n",
        "        return counts\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def type_token_ratio(tokens: list) -> float:\n",
        "\n",
        "        \"\"\"\"A function to calculate type-token ratio.\n",
        "        \n",
        "        A function to calculate the type-token ratio on the words in a string. The type-token\n",
        "        ratio is defined as the number of unique word types divided by the number\n",
        "        of total words. ATTENTION: requires the TextAnlyzer.word_counts() to run.\n",
        "\n",
        "        Args:\n",
        "            text (str): Any string\n",
        "        \n",
        "        Returns:\n",
        "            \n",
        "            float: A float expressing the comments TTR\n",
        "\n",
        "        \"\"\"\n",
        "        \n",
        "        counts = word_counts(tokens)\n",
        "\n",
        "        type_count = len(counts.keys())\n",
        "        token_count = sum(counts.values())\n",
        "\n",
        "        if token_count != 0:\n",
        "            return type_count / token_count\n",
        "        else:\n",
        "            return type_count\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def content_function_ratio(tokens: list) -> float:\n",
        "\n",
        "        \"\"\"A class method to be run on tokenized comments. It returns the content-function words ratio.\n",
        "\n",
        "        If the number of function words is equal to 0, the returned digit expresses the number of content words in the text\n",
        "\n",
        "        Args:\n",
        "            self: reference to the current instance of the class\n",
        "            tokens (list): Any list of strings\n",
        "\n",
        "        Returns:\n",
        "            float: The content-function words ratio\n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        \n",
        "        stop = set(stopwords.words('italian'))\n",
        "        \n",
        "        content = {}\n",
        "        function = {}\n",
        "\n",
        "        for word in tokens:\n",
        "            if word.lower() in stop:\n",
        "                function[word] =+ 1\n",
        "            if word.lower() not in stop:\n",
        "                content[word] =+ 1\n",
        "\n",
        "        content_count = sum(content.values())\n",
        "        function_count = sum(function.values())\n",
        "\n",
        "        if len(tokens)!=0:\n",
        "        \n",
        "            return content_count / len(tokens)\n",
        "\n",
        "        else:\n",
        "            return content_count\n",
        "\n",
        "def further_cleaning(tokens:list) -> list:\n",
        "\n",
        "    text = [item for item in tokens if item.lower() not in ['url', 'hash', 'emoji']]\n",
        "\n",
        "    return text\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "def unique_emojis(string:str) -> int:\n",
        "    count = emoji.emoji_count(string=string, unique=True)\n",
        "    return count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jisZYWlbp1Cv",
        "outputId": "4d6d4aa3-894c-4df0-8f1f-f395646b96e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ITA_FB_TRAIN=pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Thesis/FB-folder/FB-train/haspeede_FB-train.tsv', sep='\\t', header=None)\n",
        "ITA_FB_TRAIN = ITA_FB_TRAIN.rename(columns={0: 'ID', 1: 'text', 2: 'label'})\n",
        "\n",
        "ITA_FB_TEST=pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/Thesis/FB-folder/FB-test/haspeede_FB-test.tsv', sep='\\t', header=None)\n",
        "ITA_FB_TEST = ITA_FB_TEST.rename(columns={0: 'ID', 1: 'text', 2: 'label'})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "23NZSPenqHuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XjlM_TXo4ZK"
      },
      "outputs": [],
      "source": [
        "### Facebook\n",
        "\n",
        "#ITA_FB_TRAIN = pd.read_csv(\"C:/Users/Leonardo/Desktop/Thesis-Internship/Data/FB-folder/FB-train/haspeede_FB-train.tsv\", sep='\\t', header=None)\n",
        "#ITA_FB_TRAIN = ITA_FB_TRAIN.rename(columns={0: 'ID', 1: 'text', 2: 'HS'})\n",
        "\n",
        "\n",
        "#ITA_FB_TEST = pd.read_csv(\"C:/Users/Leonardo/Desktop/Thesis-Internship/Data/FB-folder/FB-test/haspeede_FB-test.tsv\", sep='\\t', header=None)\n",
        "#ITA_FB_TEST = ITA_FB_TEST.rename(columns={0: 'ID', 1: 'text', 2: 'HS'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNRrj5i5o4ZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f3c9f566-9019-448c-f098-b779ee10bc8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID                                               text\n",
              "0   1                                    Ma....anche no!\n",
              "1   2                                   Ma dove vivono ?\n",
              "2   3  Le vai a impollinare tu le piante e gli alberi...\n",
              "3   4   Ma manda li a quel paese questi zingari bugiardi\n",
              "4   5  Complimenti a chi sostiene ancora questa \"poli..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30a76f2f-bb2f-4729-a8e2-f6d974f22c5b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Ma....anche no!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Ma dove vivono ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Le vai a impollinare tu le piante e gli alberi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Ma manda li a quel paese questi zingari bugiardi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Complimenti a chi sostiene ancora questa \"poli...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30a76f2f-bb2f-4729-a8e2-f6d974f22c5b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30a76f2f-bb2f-4729-a8e2-f6d974f22c5b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30a76f2f-bb2f-4729-a8e2-f6d974f22c5b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "ITA_FB_TEST.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkBptxuTo4ZL"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['tokens'], ITA_FB_TRAIN['clean_tokens'], ITA_FB_TRAIN['clean_tokens_lower'], ITA_FB_TRAIN['tokens_lower'], ITA_FB_TRAIN['text_lower'] = zip(*ITA_FB_TRAIN['text'].map(split_textNEW))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64i1fMY6o4ZM"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['tokens'], ITA_FB_TEST['clean_tokens'], ITA_FB_TEST['clean_tokens_lower'], ITA_FB_TEST['tokens_lower'], ITA_FB_TEST['text_lower'] = zip(*ITA_FB_TEST['text'].map(split_textNEW))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdrLM2CEo4ZN"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['nr_emoticons'], ITA_FB_TRAIN['percentage_emoticons'], ITA_FB_TRAIN['emoticons'] = zip(*ITA_FB_TRAIN['tokens'].map(emoticons))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnuh6d8zo4ZO"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['nr_emoticons'], ITA_FB_TEST['percentage_emoticons'], ITA_FB_TEST['emoticons'] = zip(*ITA_FB_TEST['tokens'].map(emoticons))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daf5D6F9o4ZP"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['number_flooding_letters'],ITA_FB_TRAIN['percentage_flooding_letters'],ITA_FB_TRAIN['flooding_letters'] = zip(*ITA_FB_TRAIN['tokens'].map(letter_flooding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppk6qPwOo4ZR"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['number_flooding_letters'],ITA_FB_TEST['percentage_flooding_letters'],ITA_FB_TEST['flooding_letters'] = zip(*ITA_FB_TEST['tokens'].map(letter_flooding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTaX1rD1o4ZS"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['number_flooding_punct'], ITA_FB_TRAIN['percentage_flooding_punct'], ITA_FB_TRAIN['flooding_punctuation'], ITA_FB_TRAIN['total_punct'] = zip(*ITA_FB_TRAIN['tokens'].map(punct_flooding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nh_t_ICo4ZS"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['number_flooding_punct'], ITA_FB_TEST['percentage_flooding_punct'], ITA_FB_TEST['flooding_punctuation'], ITA_FB_TEST['total_punct'] = zip(*ITA_FB_TEST['tokens'].map(punct_flooding))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHCK3tmlo4ZT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7712968-fbb4-4b9f-bc67-954e25adaffa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    838\n",
              "1    126\n",
              "2     29\n",
              "3      5\n",
              "5      1\n",
              "4      1\n",
              "Name: number_flooding_punct, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "ITA_FB_TEST.number_flooding_punct.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOCl0f5fo4ZT"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['nr_unconvcaps'], ITA_FB_TRAIN['percentage_unconvcaps'], ITA_FB_TRAIN['unconv_caps'] = zip(*ITA_FB_TRAIN['tokens'].map(unconv_capitalization))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66Hj0HUOo4ZT"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['nr_unconvcaps'], ITA_FB_TEST['percentage_unconvcaps'], ITA_FB_TEST['unconv_caps'] = zip(*ITA_FB_TEST['tokens'].map(unconv_capitalization))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ3unZ3eo4ZT"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['nr_kisses'],ITA_FB_TRAIN['percentage_kisses'],ITA_FB_TRAIN['kisses_dict'] = zip(*ITA_FB_TRAIN['tokens'].map(kisses_and_hugs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CqszDAfo4ZU"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['nr_kisses'],ITA_FB_TEST['percentage_kisses'],ITA_FB_TEST['kisses_dict'] = zip(*ITA_FB_TEST['tokens'].map(kisses_and_hugs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0_oNuo3o4ZU"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['nr_unconv_combis'], ITA_FB_TRAIN['percentage_unconv_combis'], ITA_FB_TRAIN['unconv_combis_dict'] = zip(*ITA_FB_TRAIN['tokens'].map(unconv_combis))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl7bDv8vo4ZU"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['nr_unconv_combis'], ITA_FB_TEST['percentage_unconv_combis'], ITA_FB_TEST['unconv_combis_dict'] = zip(*ITA_FB_TEST['tokens'].map(unconv_combis))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl0g6GnHo4ZU"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['nr_leetspeak'], ITA_FB_TRAIN['percentage_leetspeak'], ITA_FB_TRAIN['leetspeak_words'] = zip(*ITA_FB_TRAIN['tokens'].map(leetspeak))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNe_RVKIo4ZV"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['nr_leetspeak'], ITA_FB_TEST['percentage_leetspeak'], ITA_FB_TEST['leetspeak_words'] = zip(*ITA_FB_TEST['tokens'].map(leetspeak))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZKSk4Aro4ZV"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['nr_abbrev_forms'], ITA_FB_TRAIN['percentage_abbrev_forms'], ITA_FB_TRAIN['abbrev_forms'] = zip(*ITA_FB_TRAIN['tokens'].map(abbrev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5Pfg8WYo4ZV"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['nr_abbrev_forms'], ITA_FB_TEST['percentage_abbrev_forms'], ITA_FB_TEST['abbrev_forms'] = zip(*ITA_FB_TEST['tokens'].map(abbrev))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qvrtbWNo4ZV"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['nr_laughter'], ITA_FB_TRAIN['percentage_laughter'], ITA_FB_TRAIN['laughter_dict'] = zip(*ITA_FB_TRAIN['tokens'].map(laughter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQHa9Jo9o4ZV"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['nr_laughter'], ITA_FB_TEST['percentage_laughter'], ITA_FB_TEST['laughter_dict'] = zip(*ITA_FB_TEST['tokens'].map(laughter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V61x8E1uo4ZW"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['nr_discourse_markers'], ITA_FB_TRAIN['discourse_markers'], ITA_FB_TRAIN['discourse_markers'] = zip(*ITA_FB_TRAIN['tokens'].map(discourse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0QMwTj4o4ZW"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TEST['nr_discourse_markers'], ITA_FB_TEST['discourse_markers'], ITA_FB_TEST['discourse_markers'] = zip(*ITA_FB_TEST['tokens'].map(discourse))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVcDpcJYo4ZW"
      },
      "source": [
        "## Lexical diversity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsIBNzfLo4ZX"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['tokens_2'] = ITA_FB_TRAIN['text'].apply(preprocessor)\n",
        "ITA_FB_TEST['tokens_2'] = ITA_FB_TRAIN['text'].apply(preprocessor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKhFYtlEo4ZX"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['tokens_2'] = ITA_FB_TRAIN['tokens_2'].apply(demojizer)\n",
        "ITA_FB_TRAIN['tokens_2'] = ITA_FB_TRAIN['tokens_2'].apply(italian_tokenizer)\n",
        "\n",
        "ITA_FB_TEST['tokens_2'] = ITA_FB_TEST['tokens_2'].apply(demojizer)\n",
        "ITA_FB_TEST['tokens_2'] = ITA_FB_TEST['tokens_2'].apply(italian_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLmsWMnro4ZX"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['comment_length'] = ITA_FB_TRAIN['tokens_2'].apply(comment_length)\n",
        "ITA_FB_TEST['comment_length'] = ITA_FB_TEST['tokens_2'].apply(comment_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbVaUspIo4ZX"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['tokens_3'] = ITA_FB_TRAIN['tokens_2'].apply(further_cleaning)\n",
        "ITA_FB_TEST['tokens_3'] = ITA_FB_TEST['tokens_2'].apply(further_cleaning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65DRWfvbo4ZX"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['CFR'] = ITA_FB_TRAIN['tokens_3'].apply(content_function_ratio)\n",
        "ITA_FB_TRAIN['TTR'] = ITA_FB_TRAIN['tokens_2'].apply(type_token_ratio)\n",
        "\n",
        "ITA_FB_TEST['CFR'] = ITA_FB_TEST['tokens_3'].apply(content_function_ratio)\n",
        "ITA_FB_TEST['TTR'] = ITA_FB_TEST['tokens_2'].apply(type_token_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775lMcdFo4ZY"
      },
      "outputs": [],
      "source": [
        "ITA_FB_TRAIN['unique_emojis'] = ITA_FB_TRAIN['text'].apply(unique_emojis)\n",
        "ITA_FB_TEST['unique_emojis'] = ITA_FB_TEST['text'].apply(unique_emojis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmYdUKb1o4ZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44feec0a-93b5-4163-f4df-c845d8735048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44 43\n"
          ]
        }
      ],
      "source": [
        "print(len(ITA_FB_TRAIN.columns), len(ITA_FB_TEST.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional colums with different characteristics\n"
      ],
      "metadata": {
        "id": "6mQcw1hUY1sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def emoji_throw_away(text):\n",
        "  text = demoji.replace(string=text, repl=' ')\n",
        "  return text\n",
        "\n",
        "ITA_FB_TRAIN['text_lower_no_emoji'] = ITA_FB_TRAIN['text_lower'].apply(emoji_throw_away)\n",
        "ITA_FB_TEST['text_lower_no_emoji'] = ITA_FB_TEST['text_lower'].apply(emoji_throw_away)"
      ],
      "metadata": {
        "id": "pCKKFMrLZKnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ITA_FB_TRAIN['text_lower_no_emoji'] = ITA_FB_TRAIN['text_lower_no_emoji'].apply(preprocessor)\n",
        "ITA_FB_TEST['text_lower_no_emoji'] = ITA_FB_TEST['text_lower_no_emoji'].apply(preprocessor)\n",
        "ITA_FB_TRAIN['text_lower_no_emoji']"
      ],
      "metadata": {
        "id": "xyiNr7sZakG8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a2ad84-0299-49cf-a0d0-0700ead1aa84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   io voterò no renzi deve andare a casa\n",
              "1                                 poi si sentono fiere eh\n",
              "2                                                   belli\n",
              "3                                                  arrusi\n",
              "4       sono indigeste fanno anche venire la colica in...\n",
              "                              ...                        \n",
              "2995                  no sono insopportabili e inquirenti\n",
              "2996    matteo iniziamo a multare pesantemente i write...\n",
              "2997      pur di prende soldi si venderebbe anche il culo\n",
              "2998         governo golpista a morte per alto tradimento\n",
              "2999                      la malpezzi non la posso vedere\n",
              "Name: text_lower_no_emoji, Length: 3000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def PNR(tokens: list) -> float:\n",
        "  r='!\"#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "  count = 0\n",
        "  for token in tokens:\n",
        "    if token in r:\n",
        "      count += 1\n",
        "\n",
        "    for char in token:\n",
        "      if char in r:\n",
        "        r='!\"#$%&amp;()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
        "        count += 1\n",
        "    \n",
        "    return count / len(tokens)\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "def emoji_throw_away(text):\n",
        "  text = demoji.replace(string=text, repl=' ')\n",
        "  return text\n",
        "\n",
        "ITA_FB_TRAIN['tokens_4'] = ITA_FB_TRAIN.text.apply(emoji_throw_away)\n",
        "ITA_FB_TRAIN['tokens_4'] = ITA_FB_TRAIN.text.apply(WordPunctTokenizer().tokenize)\n",
        "ITA_FB_TRAIN['PNR'] = ITA_FB_TRAIN.tokens_4.apply(PNR)\n",
        "\n",
        "ITA_FB_TEST['tokens_4'] = ITA_FB_TEST.text.apply(emoji_throw_away)\n",
        "ITA_FB_TEST['tokens_4'] = ITA_FB_TEST.text.apply(WordPunctTokenizer().tokenize)\n",
        "ITA_FB_TEST['PNR'] = ITA_FB_TEST.tokens_4.apply(PNR)"
      ],
      "metadata": {
        "id": "A7zvfeoL3g_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the DF for future work"
      ],
      "metadata": {
        "id": "yUYxemSuYgal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ITA_FB_TRAIN.to_csv('data_train.csv')\n",
        "!cp data_train.csv \"/content/gdrive/MyDrive/Colab Notebooks/Thesis/\""
      ],
      "metadata": {
        "id": "H5sBRMGUgccF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ITA_FB_TEST.to_csv('data_test.csv')\n",
        "!cp data_test.csv \"/content/gdrive/MyDrive/Colab Notebooks/Thesis/\""
      ],
      "metadata": {
        "id": "LXieRzUsl2Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7cy7ug6g37O-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "58a51bcd190156487426da2ad09203c2d900446cfcac933d60f788d428342e49"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}